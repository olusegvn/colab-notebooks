{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "feedforward",
      "language": "python",
      "name": "feedforward"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "English-to-German translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olusegvn/colab-notebooks/blob/main/English_to_German_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_okbgTKkzT_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19cfa435-7c6f-4d05-81e1-4b8894cc86e9"
      },
      "source": [
        "!pip install tensorflow==2.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/89/f2d29c2eafc2eeafb17d5634340e06366af904d332341200a49d954bce85/tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 42kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.12.1)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.34.1)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 35.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.12.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.0) (56.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, numpy, h5py, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VyTvxPN1iZn"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer,Input, Dense, Flatten, Conv2D, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuIi4lAR1iZt"
      },
      "source": [
        "dataset from http://www.manythings.org/anki/ to build the neural translation model. This dataset consists of over 200,000 pairs of sentences in English and German."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9Dq6vv3FVO"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset is available for download as a zip file at the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1KczOciG7sYY7SB9UlBeRP1T9659b121Q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw99tEEQ3bKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4252e03-aa3f-4801-fc28-0cc451a626fd"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/Datasets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PetPpw1iZu"
      },
      "source": [
        "# Run this cell to load the dataset\n",
        "\n",
        "NUM_EXAMPLES = 20000\n",
        "data_examples = []\n",
        "with open('deu.txt', 'r', encoding='utf8') as f:\n",
        "    for line in f.readlines():\n",
        "        if len(data_examples) < NUM_EXAMPLES:\n",
        "            data_examples.append(line)\n",
        "        else:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JumLjJ631iZy"
      },
      "source": [
        "# These functions preprocess English and German sentences\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"ü\", 'ue', sentence)\n",
        "    sentence = re.sub(r\"ä\", 'ae', sentence)\n",
        "    sentence = re.sub(r\"ö\", 'oe', sentence)\n",
        "    sentence = re.sub(r'ß', 'ss', sentence)\n",
        "    \n",
        "    sentence = unicode_to_ascii(sentence)\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r\"[^a-z?.!,']+\", \" \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    \n",
        "    return sentence.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fP7P-yK4RS7"
      },
      "source": [
        "The custom model consists of an encoder RNN and a decoder RNN. The encoder takes words of an English sentence as input, and uses a pre-trained word embedding to embed the words into a 128-dimensional space. To indicate the end of the input sentence, a special end token (in the same 128-dimensional space) is passed in as an input. This token is a TensorFlow Variable that is learned in the training phase (unlike the pre-trained word embedding, which is frozen).\n",
        "\n",
        "The decoder RNN takes the internal state of the encoder network as its initial state. A start token is passed in as the first input, which is embedded using a learned German word embedding. The decoder RNN then makes a prediction for the next German word, which during inference is then passed in as the following input, and this process is repeated until the special `<end>` token is emitted from the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9G20C4bk1iZ4"
      },
      "source": [
        "f_english = lambda x: preprocess_sentence(re.split('\\t', x)[0])   \n",
        "f_german = lambda x: '<start> '+preprocess_sentence(re.split('\\t', x)[1])+' <end>'\n",
        "english_sentences = list(map(f_english, data_examples))\n",
        "german_sentences = list(map(f_german, data_examples))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxwl-1rB1iZ8"
      },
      "source": [
        "tokenizer =  tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(german_sentences)\n",
        "tokenizer_config = tokenizer.get_config()\n",
        "word_indices = json.loads(tokenizer_config['word_index'])\n",
        "index_words = json.loads(tokenizer_config['index_word'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnIdqZFk1iaA"
      },
      "source": [
        "german_sentences_seq = tokenizer.texts_to_sequences(german_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UlnBdIK1iaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c60e0ff-fb9f-4de0-cdde-5698912fded7"
      },
      "source": [
        "idx = np.random.choice(len(english_sentences), 5, replace=False)\n",
        "print(f\"Chosen random indices: {idx}\\n\")\n",
        "for _ in idx:\n",
        "  print(english_sentences[_])\n",
        "  print(german_sentences[_])\n",
        "  print(list(map(lambda x: word_indices[x], german_sentences[_].split(' '))), end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chosen random indices: [10816  6580   666 17377 13319]\n",
            "\n",
            "i'll enjoy this .\n",
            "<start> das werde ich geniessen . <end>\n",
            "[1, 11, 39, 4, 2023, 3, 2]\n",
            "\n",
            "are they alive ?\n",
            "<start> sind sie am leben ? <end>\n",
            "[1, 23, 8, 146, 152, 7, 2]\n",
            "\n",
            "i'm a man .\n",
            "<start> ich bin ein mann . <end>\n",
            "[1, 4, 15, 19, 193, 3, 2]\n",
            "\n",
            "we depend on you .\n",
            "<start> wir sind auf sie angewiesen . <end>\n",
            "[1, 17, 23, 29, 8, 2237, 3, 2]\n",
            "\n",
            "you're arrogant .\n",
            "<start> ihr seid arrogant . <end>\n",
            "[1, 27, 73, 1311, 3, 2]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGTaqt1t1iaI"
      },
      "source": [
        "  german_sentences_seq = tf.keras.preprocessing.sequence.pad_sequences(german_sentences_seq, maxlen=None, padding='post', value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqiO7HuQ1iaL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foL7Ihs21iaP"
      },
      "source": [
        "## 2. Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywZgobCh1iaR"
      },
      "source": [
        "# Load embedding module from Tensorflow Hub\n",
        "\n",
        "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\", \n",
        "                                 output_shape=[128], input_shape=[], dtype=tf.string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiY8QEDp1iaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e562cb-26a5-45b6-bae2-a1d31ca035ac"
      },
      "source": [
        "# Test the layer\n",
        "\n",
        "embedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([7, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BUJOl_1iaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfcadeb5-e858-42f1-cef3-b257506b4537"
      },
      "source": [
        "# Create a random training and validation set split of the data, reserving e.g. 20% of the data \n",
        "# for validation (NB: each English dataset example is a single sentence string, and each German dataset example is a sequence of padded integer tokens).\n",
        "\n",
        "\n",
        "# Load the training and validation sets into a tf.data.Dataset object, passing in a tuple of English and German data for both training and validation sets.\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(english_sentences, german_sentences_seq, test_size=0.2, shuffle=True)\n",
        "\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "print(training_dataset.element_spec)\n",
        "print(validation_dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n",
            "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6rM8Bl1iad"
      },
      "source": [
        "# Create a function to map over the datasets that splits each English sentence at spaces. Apply this function\n",
        "#  to both Dataset objects using the map method. Hint: look at the tf.strings.split function.\n",
        "\n",
        "def map_split(x,y):\n",
        "  return tf.strings.split(x, ' '),  y\n",
        "training_dataset = training_dataset.map(map_split)\n",
        "validation_dataset = validation_dataset.map(map_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7bn3mRs1iaj"
      },
      "source": [
        "# Create a function to map over the datasets that embeds each sequence of English words using the loaded embedding layer/model. \n",
        "# Apply this function to both Dataset objects using the map method.\n",
        "def map_embeddings_func(x,y):\n",
        "  return embedding_layer(x), y\n",
        "  \n",
        "training_dataset = training_dataset.map(map_embeddings_func)\n",
        "validation_dataset = validation_dataset.map(map_embeddings_func)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Fdso381ian",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767728c9-218b-41e6-f0e1-a62f90c1ce14"
      },
      "source": [
        "# Create a function to filter out dataset examples where the English sentence is greater than or equal to than 13 (embedded) tokens in length.\n",
        "#  Apply this function to both Dataset objects using the filter method.\n",
        "\n",
        "\n",
        "def dataset_filter(x, y):\n",
        "  return len(x) >= tf.constant(13)\n",
        "\n",
        "training_dataset = training_dataset.filter(dataset_filter)\n",
        "validation_dataset = validation_dataset.filter(dataset_filter)\n",
        "print(training_dataset.element_spec)\n",
        "print(validation_dataset.element_spec)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n",
            "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqlxuqZ1ias"
      },
      "source": [
        "\n",
        "\n",
        "# Create a function to map over the datasets that pads each English sequence of embeddings with some distinct padding value before the sequence, \n",
        "#  so that each sequence is length 13. Apply this function to both Dataset objects using the map method. Hint: look at the tf.pad function. You can extract a Tensor shape using tf.shape; you might also find the tf.math.maximum function useful.\n",
        "\n",
        "\n",
        "def pad_embeddings(x,y):\n",
        "  return tf.pad(x,paddings=[[tf.constant(13)-len(x),0],[0,0]], mode='CONSTANT', constant_values=0), y\n",
        "\n",
        "\n",
        "training_dataset = training_dataset.map(pad_embeddings)\n",
        "validation_dataset = validation_dataset.map(pad_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdqSXEoX1iav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed5af56-fd42-4e83-db0d-88a47057286f"
      },
      "source": [
        "\n",
        "# Batch both training and validation Datasets with a batch size of 16.\n",
        "\n",
        "# Print the element_spec property for the training and validation Datasets.\n",
        "\n",
        "\n",
        "training_dataset = training_dataset.batch(16)\n",
        "validation_dataset = validation_dataset.batch(16)\n",
        "\n",
        "print(training_dataset.element_spec)\n",
        "print(validation_dataset.element_spec)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 14), dtype=tf.int32, name=None))\n",
            "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 14), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcTE_IQuR3V6"
      },
      "source": [
        "# Using the Dataset .take(1) method, print the shape of the English data example from the training Dataset.\n",
        "\n",
        "# Using the Dataset .take(1) method, print the German data example Tensor from the validation Dataset.\n",
        "\n",
        "# print(next(iter(training_dataset.take(1)))[0].shape)\n",
        "# print(next(iter(training_dataset.take(1)))[1].shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg9hjZz11ia0"
      },
      "source": [
        "# Using layer subclassing, create a custom layer that takes a batch of English data examples from one of the Datasets, \n",
        "#  and adds a learned embedded ‘end’ token to the end of each sequence.\n",
        "\n",
        "# This layer should create a TensorFlow Variable (that will be learned during training) that is 128-dimensional (the size of the embedding space).\n",
        "#  Hint: you may find it helpful in the call method to use the tf.tile function to replicate the end token embedding across every element in the batch.\n",
        "\n",
        "# Using the Dataset .take(1) method, extract a batch of English data examples from the training Dataset and print the shape. Test the custom layer by calling the layer on the English data batch Tensor and print the resulting Tensor shape (the layer should increase the sequence length by one).\n",
        "\n",
        "class MyLayer(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyLayer, self).__init__(**kwargs)\n",
        "    self.learned_end= tf.Variable(initial_value=tf.zeros(shape=(1,128)))\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    self.learned_end = tf.tile(self.learned_end, [tf.shape(inputs)[0],1])\n",
        "    self.learned_end = tf.expand_dims(self.learned_end, axis=1, name=None)\n",
        "    return tf.keras.layers.Concatenate(axis=1)([inputs, self.learned_end])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAd3i4_y1ia-"
      },
      "source": [
        "## 4. Build the encoder network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJMWJtUXYnm"
      },
      "source": [
        "inputs = Input(batch_shape=(None, 13, 128))\n",
        "x = MyLayer()(inputs)\n",
        "x = tf.keras.layers.Masking(mask_value=0, name='masking_layer')(x)\n",
        "x,hidden_s,cell_s = tf.keras.layers.LSTM(units=512, return_state = True, return_sequences=True, name='LSTM_states')(x)\n",
        "encoder_model = Model(inputs = inputs, outputs= [hidden_s,cell_s], name = 'encoder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R2LqbfV1ia_"
      },
      "source": [
        "# inputs = Input(batch_shape=(None, 13, 128))\n",
        "\n",
        "# def return_my_encoeder_model():\n",
        "#   my_layer = MyLayer()\n",
        "#   x = my_layer(inputs)\n",
        "#   x = tf.keras.layers.Masking(mask_value=0, name='masking')(x)\n",
        "#   x,hidden_s,cell_s = tf.keras.layers.LSTM(units=512, return_state = True, name='LSTM_states')(x)\n",
        "#   del my_layer\n",
        "#   return Model(inputs = inputs, outputs= [hidden_s,cell_s])\n",
        "\n",
        "# MyEncoderModel = return_my_encoeder_model()\n",
        "# MyEncoderModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0MglKQeUpEt"
      },
      "source": [
        "# batch = next(iter(training_dataset.take(1)))[0]\n",
        "# print(batch.shape)\n",
        "# [print(_.shape) for _ in encoder_model(batch)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5XW6NxL1ibC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14d22a3-6cbf-4592-a9b1-b2b17993d04d"
      },
      "source": [
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 13, 128)]         0         \n",
            "_________________________________________________________________\n",
            "my_layer (MyLayer)           (None, 14, 128)           0         \n",
            "_________________________________________________________________\n",
            "masking_layer (Masking)      (None, 14, 128)           0         \n",
            "_________________________________________________________________\n",
            "LSTM_states (LSTM)           [(None, 14, 512), (None,  1312768   \n",
            "=================================================================\n",
            "Total params: 1,312,768\n",
            "Trainable params: 1,312,768\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEk9ikVh1ibL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l50qhnXD1ibT"
      },
      "source": [
        "class Decoder(Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    # Define layers\n",
        "    self.layer_1 = tf.keras.layers.Embedding(input_dim = len(tokenizer.word_index)+1, mask_zero=True, output_dim=128) \n",
        "    self.layer_2= tf.keras.layers.LSTM(units=512,return_sequences=True, return_state=True)\n",
        "    self.layer_3 = Dense(units = len(tokenizer.word_index) + 1)\n",
        "  \n",
        "  def call(self, inputs, hidden_state=None, cell_state=None):\n",
        "    # Define forward pass\n",
        "    x = self.layer_1(inputs)\n",
        "    if hidden_state is not None and cell_state is not None:\n",
        "      x,hidden_s,cell_s = self.layer_2(x,initial_state = [hidden_state,cell_state])\n",
        "    else:\n",
        "      x,hidden_s,cell_s = self.layer_2(x)\n",
        "    x = self.layer_3(x)\n",
        "    return x,hidden_s,cell_s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBxyvYmrl02A"
      },
      "source": [
        "# next(iter(training_dataset.take(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG5CRIXV1ibi"
      },
      "source": [
        "decoder = Decoder()\n",
        "# uncomment the following line\n",
        "# hidden,cell = encoder_model(next(iter(training_dataset.take(1)))[0])\n",
        "\n",
        "for english_sample, german_example in training_dataset.take(1):\n",
        "  hidden_state, cell_state = encoder_model(english_sample)\n",
        "  decoder_output, hidden_state, cell_state = decoder(german_example)\n",
        "\n",
        "  print(f'encoder state shape: {hidden_state}')\n",
        "  print(f'decoder output shape: {decoder_output}')\n",
        "  print(f'cell state shape: {cell_state}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvqqT_ET1ibl"
      },
      "source": [
        "# decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pST9XGJ81ibo"
      },
      "source": [
        "## 6. Custom training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJHbWqs1ibr"
      },
      "source": [
        "def get_german_data(data):\n",
        "  return german_data[:, :-1], german_data[:, 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EgoTN_itoPP"
      },
      "source": [
        "optimizer= tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "variables = encoder_model.trainable_variables + decoder.trainable_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvu4J4-u1ibu"
      },
      "source": [
        "@tf.function\n",
        "def grad_fn(english_inputs, german_inputs, German_outputs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    hidden_state, cell_state = encoder_model(english_inputs)\n",
        "    decoder_outputs, *_ = decoder(german_inputs,hidden_state=hidden_state,cell_state=cell_state)\n",
        "    loss_value = tf.keras.losses.SparseCategoricalCrossentropy(German_outputs,decoder_outputs, from_logits=True)\n",
        "    return loss_value, tape.gradient(loss_value, variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl5H06811ibx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6c77a3-aeee-4fd7-caf6-43bb7c5524d2"
      },
      "source": [
        "  train_loss = []\n",
        "  val_loss_results = []   \n",
        "  for epoch in range(5):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    val_epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    print('a\\n\\n')\n",
        "    for english_input, german_data in training_dataset:\n",
        "      german_input, german_output = get_german_data(german_data)\n",
        "      loss_value, grads= grad_fn(english_input, german_input, german_output)\n",
        "      print(loss_value+'\\n\\n')\n",
        "      optimizer.apply_gradients(zip(grads, variables))\n",
        "      epoch_loss_avg(loss_value)\n",
        "    for english_input, german_data in validation_dataset:\n",
        "      german_input, german_output = get_german_data(german_data)\n",
        "      hidden_state, cell_state = encoder_model(english_inptut)\n",
        "      decoder_output, *_ = decoder_network(german_input, hidden_state=hidden_state, cell_state=cell_state)\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(german_output,decoder_outputs, from_logits=True)\n",
        "      val_epoch_loss_avg(loss)\n",
        "    train_loss.append(epoch_loss_avg.result())\n",
        "    val_loss_results.append(val_epoch_loss_avg.result())\n",
        "    print(f\" Epoch :{epoch}: Loss: {train_loss[0]} : Val_Loss: {val_loss_results[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWOYJ3l1ib1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VSmBdwT1ib4"
      },
      "source": [
        "fig, axis = plt.subplots(1, 2, sharex=True, figsize=(12, 5))\n",
        "axis[0].set_xlabel('epochs')\n",
        "axis[0].set_ylabel('Training loss')\n",
        "axis[0].plot(train_loss_results)\n",
        "\n",
        "axis[1].set_xlabel('epochs')\n",
        "axis[1].set_ylabel('Validaion loss')\n",
        "axis[1].plot(val_loss_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpacst2F1ib7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM2gvBM11ib-"
      },
      "source": [
        "## 7. Use the model to translate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnQtE7L1icA"
      },
      "source": [
        "num_samples = 5\n",
        "inx = np.random.choice(len(english_sentences), num_samples, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohleJRcJ1icD"
      },
      "source": [
        "for i,Chunk in enumerate(np.array(english_sentences)[inx]):\n",
        "  result = []\n",
        "  chunk2 = tf.strings.split(Chunk,' ')\n",
        "  chunk2 = embedding_layer(chunk2)\n",
        "  if chunk2.shape[0] >13:\n",
        "    print(\"More than 13 embedded Tokens---Skipping\")\n",
        "    continue\n",
        "  else:\n",
        "    chunk2 = tf.pad(chunk2,paddings=[[13-tf.shape(chunk2)[0],0],[0,0]], mode='CONSTANT', constant_values=0)\n",
        "    abs= tf.expand_dims(chunk2, axis=0)\n",
        "    hidden_stat,cell_stat = encoder_model(chunk2)\n",
        "    decoder_input = tf.expand_dims([word_indexs.get('<start>')],0)\n",
        "    for t in range(training_dataset.element_spec[1].shape[1]):\n",
        "      out,hidden_stat,cell_stat = decoder(decoder_input,hidden_stat,cell_stat)\n",
        "      [[predicted_id]] = tf.argmax(out, axis=2).numpy()\n",
        "      result.append(predicted_id)\n",
        "      if index_words.get(predicted_id) == '<end>':\n",
        "        break\n",
        "      # the predicted ID is fed back into the model\n",
        "      decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "    result = tokenizer.sequences_to_texts([result])\n",
        "    print(f\"Sample :{i+1} English Sentence: {chunk} German Translation: {result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unk60cEy1icI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}